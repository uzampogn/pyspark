{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PySpark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPrQudSLPDcRJhi5qKmmTuj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uzampogn/pyspark/blob/main/PySpark%20intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources:\n",
        "\n",
        "* https://sparkbyexamples.com/"
      ],
      "metadata": {
        "id": "kyydp0KMBF1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Big Data Fundamentals with PySpark"
      ],
      "metadata": {
        "id": "Bjujg5J0BNm8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding SparkContext\n",
        "A SparkContext represents the entry point to Spark functionality. It's like a key to your car. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. PySpark automatically creates a SparkContext for you in the PySpark shell (so you don't have to create it by yourself) and is exposed via a variable sc.\n",
        "\n",
        "In this simple exercise, you'll find out the attributes of the SparkContext in your PySpark shell which you'll be using for the rest of the course.\n",
        "\n",
        "sc = SparkContext\n",
        "\n",
        "sc.version spark version\n",
        "\n",
        "sc.pythonVer Python ver\n",
        "\n",
        "sc.master "
      ],
      "metadata": {
        "id": "hrJP3JABpvO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RDD = Resilient Distributed Datasets\n",
        "\n",
        "*   Resilient = ability to cope with failures\n",
        "*   Distributed = spread across various machines\n",
        "*   Collect of partitioned data e.g. arrays, tables, tuples, etc\n",
        "\n",
        "Create/load RDDs:\n",
        "\n",
        "*   sc.parallelize(list)\n",
        "*   sc.textFile(path)\n",
        "\n",
        "Check RDD partitions:\n",
        "*   rdd.getNumPartitions()\n",
        "\n",
        "Operations of RDD Transformation vs Action:\n",
        "* Transformation return a new RDD\n",
        "  > Transformation follows a lazy evaluation\n",
        "\n",
        "  > example: map(), filter(), flatMap(returns multiple values for each element), union()\n",
        "\n",
        "  > join()\n",
        "\n",
        "* Action perform computation on RDD\n",
        "  > collect() return all element of data as array\n",
        "\n",
        "  > take(x) return an array with first x elements of the datasets\n",
        "\n",
        "  > first() return first element\n",
        "\n",
        "  > count() return total number of elements\n",
        "\n",
        "  > reduce()\n",
        "\n",
        "  > saveAsTextFile()\n",
        "\n",
        "  > sc.coalesce(1).saveAsTextFile() save in 1 partition\n",
        "  \n",
        "\n",
        "Pair RDDs = RDD for pair/value data structure\n",
        "Specific Transformation\n",
        "  > reduceByKey()\n",
        "  >> Apply reduceByKey() operation to sum value of unique key\n",
        "\n",
        "  >> Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "  > sortByKey()\n",
        "\n",
        "  > groupByKey()\n",
        "\n",
        "Specific action\n",
        "  > countByKey()\n",
        "  \n",
        "  > collectAsMap() return k,v pair as a dictionary\n"
      ],
      "metadata": {
        "id": "Th1gsEFns87s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataframes in Pyspark\n",
        "\n",
        "Create:\n",
        "* sc.createDataFrame(rdd)\n",
        "* spark.read.csv(file_path)\n",
        "\n",
        "Transformations:\n",
        "* select()\n",
        "* filter(df.Age < 21)\n",
        "* groupby()\n",
        "* orderby()\n",
        "* dropDuplicates()\n",
        "* withColumnRenamed()\n",
        "\n",
        "Actions:\n",
        "* printSchema()\n",
        "* head()\n",
        "* show() print first 20 rows by default\n",
        "* count()\n",
        "* columns\n",
        "* describe()\n",
        "* info()\n",
        "\n",
        "\n",
        "Create a temp table from a dataframe:\n",
        "* people_df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "Query with spark sql:\n",
        "* new_df = spark.sql(query)\n",
        "\n",
        "Visualization in Pyspark:\n",
        "* pyspark_dist_explore - Plotting library\n",
        "> hist()\n",
        "\n",
        "  > distplot()\n",
        "\n",
        "  > pandas_histogram()\n",
        "\n",
        "* toPandas()\n",
        "* HandySpark library\n",
        "> .toHandy()"
      ],
      "metadata": {
        "id": "mFVB1qTgs84x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Snippet"
      ],
      "metadata": {
        "id": "VnYCbIDurkHe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMB7Z_rEpC3D"
      },
      "outputs": [],
      "source": [
        "# Display the first 10 words and their frequencies from the input RDD\n",
        "for word in resultRDD.take(10):\n",
        "\tprint(word)\n",
        "\n",
        "# Swap the keys and values from the input RDD\n",
        "resultRDD_swap = resultRDD.map(lambda x: (x[1], x[0]))\n",
        "\n",
        "# Sort the keys in descending order\n",
        "resultRDD_swap_sort = resultRDD_swap.sortByKey(ascending=False)\n",
        "\n",
        "# Show the top 10 most frequent words and their frequencies from the sorted RDD\n",
        "for word in resultRDD_swap_sort.take(10):\n",
        "\tprint(\"{},{}\". format(word[1], word[0]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary view of fifa_df\n",
        "fifa_df.createOrReplaceTempView('fifa_df_table')\n",
        "\n",
        "# Construct the \"query\"\n",
        "query = '''SELECT Age FROM fifa_df_table WHERE Nationality == \"Germany\"'''\n",
        "\n",
        "# Apply the SQL \"query\"\n",
        "fifa_df_germany_age = spark.sql(query)\n",
        "\n",
        "# Generate basic statistics\n",
        "fifa_df_germany_age.describe().show()"
      ],
      "metadata": {
        "id": "ikS2v1e3s8Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning data using Pyspark"
      ],
      "metadata": {
        "id": "FeeBoCjsx_8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pyspark Dataframe propreties:\n",
        "\n",
        "\n",
        "*   Immutable\n",
        "\n",
        "  >Once created, an object can't be changed. If a transformation is applied to an object, the data will be overwritten.\n",
        "\n",
        "  >Spark takes advantage of data immutability to efficiently share / create new data representations throughout the cluster.\n",
        "\n",
        "\n",
        "\n",
        "*   Lazy processing\n",
        "  \n",
        "  >\"Nothing (Transformation) happen till an action is performed\" = Transformation are lazy\n",
        "\n",
        "  > Lazy processing operations will usually return in about the same amount of time regardless of the actual quantity of data. ** Spark does not perform any transformations until an action is requested.**\n",
        "\n",
        "  > NB: Transformation can be re-ordered for best performance\n",
        "\n",
        "* Parquet vs Csv\n",
        "\n",
        "  > Parquest has schema on read properties. Allowing to filter before processing.\n",
        "\n",
        "  > Spark will only process the data necessary to complete the operations you define versus reading the entire dataset. This gives Spark more flexibility in accessing the data and often drastically improves performance on large datasets.\n"
      ],
      "metadata": {
        "id": "E3qWKMtlzPj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load, read, write"
      ],
      "metadata": {
        "id": "wTdbwpCxSGYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the pyspark.sql.types library\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Define a new schema using the StructType method\n",
        "people_schema = StructType([\n",
        "  # Define a StructField for each field\n",
        "  StructField('name', StringType(), False),\n",
        "  StructField('age', IntegerType(), False),\n",
        "  StructField('city', StringType(), False)\n",
        "])"
      ],
      "metadata": {
        "id": "SHP1Ev8VyCcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file\n",
        "aa_dfw_df = spark.read.format(\"csv\").options(Header=True).load('AA_DFW_2018.csv.gz')\n",
        "\n",
        "# Add the airport column using the F.lower() method\n",
        "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
        "\n",
        "# Drop the Destination Airport column\n",
        "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
        "\n",
        "# Show the DataFrame\n",
        "aa_dfw_df.show()"
      ],
      "metadata": {
        "id": "6hxxoCbB19PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the row count of df1 and df2\n",
        "print(\"df1 Count: %d\" % df1.count())\n",
        "print(\"df2 Count: %d\" % df2.count())\n",
        "\n",
        "# Combine the DataFrames into one\n",
        "df3 = df1.union(df2)\n",
        "\n",
        "# Save the df3 DataFrame in Parquet format\n",
        "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
        "\n",
        "# Read the Parquet file into a new DataFrame and run a count\n",
        "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
      ],
      "metadata": {
        "id": "HZuvuJ5n4JPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Parquet file into flights_df\n",
        "flights_df = spark.read.parquet(\"AA_DFW_ALL.parquet\")\n",
        "\n",
        "# Register the temp table\n",
        "flights_df.createOrReplaceTempView('flights')\n",
        "\n",
        "# Run a SQL query of the average flight duration\n",
        "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
        "print('The average flight time is: %d' % avg_duration)"
      ],
      "metadata": {
        "id": "tVGNuuMy44j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter & Create new Column"
      ],
      "metadata": {
        "id": "rmbu-TkiSKGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the distinct VOTER_NAME entries\n",
        "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n",
        "\n",
        "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
        "voter_df = voter_df.where('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
        "\n",
        "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
        "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
        "\n",
        "# Show the distinct VOTER_NAME entries again\n",
        "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)"
      ],
      "metadata": {
        "id": "FXnCa49u89AS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new column called splits separated on whitespace\n",
        "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
        "\n",
        "# Create a new column called first_name based on the first item in splits\n",
        "voter_df = voter_df.withColumn(\"first_name\", voter_df.splits.getItem(0))\n",
        "\n",
        "# Get the last entry of the splits list and create a column called last_name\n",
        "voter_df = voter_df.withColumn(\"last_name\", voter_df.splits.getItem(F.size('splits') - 1))\n",
        "\n",
        "# Drop the splits column\n",
        "voter_df = voter_df.drop('splits')\n",
        "\n",
        "# Show the voter_df DataFrame\n",
        "voter_df.show()"
      ],
      "metadata": {
        "id": "xA6tuAfg-yOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column to voter_df for any voter with the title **Councilmember**\n",
        "voter_df = voter_df.withColumn('random_val',\n",
        "                               F.when(voter_df.TITLE == 'Councilmember', F.rand()))\n",
        "\n",
        "# Show some of the DataFrame rows, noting whether the when clause worked\n",
        "voter_df.show()"
      ],
      "metadata": {
        "id": "vtT39KDDArbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONDITIONAL STEP : IF THEN ELSE equivalent"
      ],
      "metadata": {
        "id": "QIKG8de_R4U2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column to voter_df for a voter based on their position\n",
        "voter_df = voter_df.withColumn('random_val',\n",
        "                               F.when(voter_df.TITLE == 'Councilmember', F.rand())\n",
        "                               .when(voter_df.TITLE == 'Mayor', 2)\n",
        "                               .otherwise(0)\n",
        "                               )\n",
        "\n",
        "# Show some of the DataFrame rows\n",
        "voter_df.show()\n",
        "\n",
        "# Use the .filter() clause with random_val\n",
        "voter_df.filter(voter_df.random_val == 0).show()"
      ],
      "metadata": {
        "id": "MpjLlD8qCBNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UDF : USER DEFINE FUNCTION"
      ],
      "metadata": {
        "id": "vCiFkRW8R0uo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getFirstAndMiddle(names):\n",
        "  # Return a space separated string of names\n",
        "  return ' '.join(names[:-1])\n",
        "\n",
        "# Define the method as a UDF\n",
        "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
        "\n",
        "# Create a new column using your UDF\n",
        "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
        "\n",
        "# Show the DataFrame\n",
        "voter_df.show() "
      ],
      "metadata": {
        "id": "modMY6seE9RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create id in distributed system"
      ],
      "metadata": {
        "id": "ckCWHf1_RuD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select all the unique council voters\n",
        "voter_df = df.select(df[\"VOTER NAME\"]).distinct()\n",
        "\n",
        "# Count the rows in voter_df\n",
        "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
        "\n",
        "# Add a ROW_ID\n",
        "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
        "\n",
        "# Show the rows with 10 highest IDs in the set\n",
        "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
      ],
      "metadata": {
        "id": "MbxGd1TwHzNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of partitions in each DataFrame\n",
        "print(\"\\nThere are %d partitions in the voter_df DataFrame.\\n\" % voter_df.rdd.getNumPartitions())\n",
        "print(\"\\nThere are %d partitions in the voter_df_single DataFrame.\\n\" % voter_df_single.rdd.getNumPartitions())\n",
        "\n",
        "# Add a ROW_ID field to each DataFrame\n",
        "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
        "voter_df_single = voter_df_single.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
        "\n",
        "# Show the top 10 IDs in each DataFrame \n",
        "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)\n",
        "voter_df_single.orderBy(voter_df_single.ROW_ID.desc()).show(10)\n"
      ],
      "metadata": {
        "id": "ulh473GqJIF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the highest ROW_ID and save it in previous_max_ID\n",
        "previous_max_ID = voter_df_march.select('ROW_ID').rdd.max()[0]\n",
        "\n",
        "# Add a ROW_ID column to voter_df_april starting at the desired value\n",
        "voter_df_april = voter_df_april.withColumn('ROW_ID', F.monotonically_increasing_id() + previous_max_ID)\n",
        "\n",
        "# Show the ROW_ID from both DataFrames and compare\n",
        "voter_df_march.select('ROW_ID').show()\n",
        "voter_df_april.select('ROW_ID').show()"
      ],
      "metadata": {
        "id": "vKcp8nweKNwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caching\n",
        "\n",
        "action of storing the dataframe in memory or on disk\n",
        "\n",
        "Advantages:\n",
        "\n",
        "   * Improve speed on later transformations & actions as the data doesn't need to be retrieved from the original data source\n",
        "   * Reduce overall usage of cluster. No need to use network & cpu as data is already in memory\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "  * Large dataset may not fit in memory\n",
        "  * Cached objects may not be available\n",
        "\n",
        "\n",
        "Tips:\n",
        "\n",
        "  * Cache only if you need dataframe again\n",
        "  * Try caching df at different moment and check performance\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "    Counting 139358 rows took 2.838447 seconds\n",
        "    Counting 139358 rows again took 1.051270 seconds\n",
        "\n",
        "Consider why the first run takes longer even though you've told it to cache() the DataFrame. Remember that even though you've applied the caching transformation, it doesn't take effect until an action is run. The action instantiates the caching after the distinct() function completes. The second time, there is no need to recalculate anything so it returns almost immediately."
      ],
      "metadata": {
        "id": "gE-XUUTqkK3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine if departures_df is in the cache\n",
        "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)\n",
        "print(\"Removing departures_df from cache\")\n",
        "\n",
        "# Remove departures_df from the cache\n",
        "departures_df.unpersist()\n",
        "\n",
        "# Check the cache status again\n",
        "print(\"Is departures_df cached?: %s\" % departures_df.is_cached)"
      ],
      "metadata": {
        "id": "_8wwmiL2naB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improve import performance\n",
        "\n",
        "First it's important to understand a Spark cluster. It's composed of:\n",
        "\n",
        "  * Driver process\n",
        "    * Handle task assignement & consolidation of data back from workers\n",
        "  * Worker process\n",
        "    * Handle transformation & action steps\n",
        "    * Once assign a task, workers are fairly independent\n",
        "\n",
        "Import performance:\n",
        "\n",
        "  * Better many small files than a large one\n",
        "  * Better if files are roughly the same size\n",
        "  * Both previous step let Spark decide how best to read the data\n",
        "  * Define Schema"
      ],
      "metadata": {
        "id": "gjIhxy7Ko7Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the number of partitions in variable\n",
        "before = departures_df.rdd.getNumPartitions()\n",
        "\n",
        "# Configure Spark to use 500 partitions\n",
        "spark.conf.set('spark.sql.shuffle.partitions', 500)\n",
        "\n",
        "# Recreate the DataFrame using the departures data file\n",
        "departures_df = spark.read.csv('departures.txt.gz').distinct()\n",
        "\n",
        "# Print the number of partitions for each instance\n",
        "print(\"Partition count before change: %d\" % before)\n",
        "print(\"Partition count after change: %d\" % departures_df.rdd.getNumPartitions())"
      ],
      "metadata": {
        "id": "iBoD8QJFneKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Performance improvements\n",
        "\n",
        "Check Spark the query plan with .explain(). It will help validate your configuration without actually running the tasks.\n",
        "\n",
        "<br>\n",
        "\n",
        "Shuffling refers to moving data around to various workers to complete a task\n",
        "\n",
        "Side effect of shuffling:\n",
        "\n",
        "  * Lower throughput of the system as workers are busy transferring data among the network\n",
        "\n",
        "How to limit shuffling:\n",
        "  * limit the use of .repartition(num_partitions)\n",
        "    > It triggers a reshuffling of the entire datasets\n",
        "\n",
        "    > Instead use .coalesce(num_partitions) to merge several partitions together\n",
        "  * watch out for .join()\n",
        "  * if possible use .broadcast() \n",
        "    > Provide a copy of an object to each worker\n",
        "\n",
        "    > To used when one dataframe is much smaller than the other"
      ],
      "metadata": {
        "id": "ofxrHzzZxsCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# Count the number of rows in the normal DataFrame\n",
        "normal_count = normal_df.count()\n",
        "normal_duration = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "# Count the number of rows in the broadcast DataFrame\n",
        "broadcast_count = broadcast_df.count()\n",
        "broadcast_duration = time.time() - start_time\n",
        "\n",
        "# Print the counts and the duration of the tests\n",
        "print(\"Normal count:\\t\\t%d\\tduration: %f\" % (normal_count, normal_duration))\n",
        "print(\"Broadcast count:\\t%d\\tduration: %f\" % (broadcast_count, broadcast_duration))"
      ],
      "metadata": {
        "id": "SAftuHk_03_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Normal count:\t\t119910\tduration: 2.819368\n",
        "    Broadcast count:\t119910\tduration: 1.102918\n",
        "\n",
        "While the difference in time is miniscule for our example, the ratio between the durations is significant. Depending on the makeup of the data being joined, you can notably cut the run time for Spark operations."
      ],
      "metadata": {
        "id": "XkzLKUkW05J3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data pipeline\n",
        "\n",
        "General steps:\n",
        "\n",
        "1.   Input: CSV, JSON, API, etc\n",
        "2.   Transformation\n",
        "3.   Output\n",
        "4.   Validation\n",
        "5.   Analysis (compute certain metric to ease data consumption)\n",
        "\n"
      ],
      "metadata": {
        "id": "UVfXa6I9yOMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the data to a DataFrame\n",
        "departures_df = spark.read.csv(\"2015-departures.csv.gz\", header=True)\n",
        "\n",
        "departures_df.printSchema()\n",
        "\n",
        "# Remove any duration of 0\n",
        "departures_df = departures_df.where('\"Actual elapsed time (Minutes)\" > 0')\n",
        "\n",
        "# Add an ID column\n",
        "departures_df = departures_df.withColumn('id', F.monotonically_increasing_id())\n",
        "\n",
        "# Write the file out to JSON format\n",
        "departures_df.write.json('output.json', mode='overwrite')"
      ],
      "metadata": {
        "id": "Gc0qlPiR1EHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the file to a DataFrame and perform a row count\n",
        "annotations_df = spark.read.csv('annotations.csv.gz', sep='|')\n",
        "full_count = annotations_df.count()\n",
        "\n",
        "# Count the number of rows beginning with '#'\n",
        "comment_count = annotations_df.where(col('_c0').startswith('#')).count()\n",
        "\n",
        "# Import the file to a new DataFrame, without commented rows\n",
        "no_comments_df = spark.read.csv('annotations.csv.gz', sep='|', comment='#')\n",
        "# Handling commented rows is easy in Spark and allows you to quickly remove any row beginning with a defined character. \n",
        "\n",
        "# Count the new DataFrame and verify the difference is as expected\n",
        "no_comments_count = no_comments_df.count()\n",
        "print(\"Full count: %d\\nComment count: %d\\nRemaining count: %d\" % (full_count, comment_count, no_comments_count))"
      ],
      "metadata": {
        "id": "ZANJfmd64ZDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split _c0 on the tab character and store the list in a variable\n",
        "tmp_fields = F.split(annotations_df['_c0'], '\\t')\n",
        "\n",
        "# Create the colcount column on the DataFrame\n",
        "annotations_df = annotations_df.withColumn('colcount', F.size(tmp_fields))\n",
        "\n",
        "# Remove any rows containing fewer than 5 fields\n",
        "annotations_df_filtered = annotations_df.filter(~ (annotations_df[\"colcount\"] < 5))\n",
        "\n",
        "# Count the number of rows\n",
        "final_count = annotations_df_filtered.count()\n",
        "print(\"Initial count: %d\\nFinal count: %d\" % (initial_count, final_count))"
      ],
      "metadata": {
        "id": "CTflyo4q7EKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the content of _c0 on the tab character (aka, '\\t')\n",
        "split_cols = F.split(annotations_df['_c0'], '\\t')\n",
        "\n",
        "# Add the columns folder, filename, width, and height\n",
        "split_df = annotations_df.withColumn('folder', split_cols.getItem(0))\n",
        "split_df = split_df.withColumn('filename', split_cols.getItem(1))\n",
        "split_df = split_df.withColumn('width', split_cols.getItem(2))\n",
        "split_df = split_df.withColumn('height', split_cols.getItem(3))\n",
        "\n",
        "# Add split_cols as a column\n",
        "split_df = split_df.withColumn('split_cols', split_cols)"
      ],
      "metadata": {
        "id": "32wCH9SQ8x-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retriever(cols, colcount):\n",
        "  # Return a list of dog data\n",
        "  return cols[4:colcount]\n",
        "\n",
        "# Define the method as a UDF\n",
        "udfRetriever = F.udf(retriever, ArrayType(StringType()))\n",
        "\n",
        "# Create a new column using your UDF\n",
        "split_df = split_df.withColumn('dog_list', udfRetriever(split_df.split_cols, split_df.colcount))\n",
        "\n",
        "# Remove the original column, split_cols, and the colcount\n",
        "split_df = split_df.drop('_c0').drop('split_cols').drop('colcount')"
      ],
      "metadata": {
        "id": "Yem89Zzw_lzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation\n",
        "\n",
        "Filter out the rows using a join:\n",
        "\n",
        "> using joins in this fashion drastically simplifies a validation task if your data permits it. The validation data doesn't necessarily need to be loaded from a file - it could be calculated on the fly, or based on a previous dataset. Optimizing these tasks will improve your overall data cleaning process. Note: There are multiple ways to define the join statement. As both DataFrames have a column with the name 'folder', Spark handles this for us automatically."
      ],
      "metadata": {
        "id": "6hyXPPD2w9Ng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Rename the column in valid_folders_df\n",
        "valid_folders_df = valid_folders_df.withColumnRenamed('_c0','folder')\n",
        "\n",
        "# Count the number of rows in split_df\n",
        "split_count = split_df.count()\n",
        "\n",
        "# Join the DataFrames\n",
        "joined_df = split_df.join(F.broadcast(valid_folders_df), \"folder\")\n",
        "\n",
        "# Compare the number of rows remaining\n",
        "joined_count = joined_df.count()\n",
        "print(\"Before: %d\\nAfter: %d\" % (split_count, joined_count))"
      ],
      "metadata": {
        "id": "rlPpkLffw_Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the row counts for each DataFrame\n",
        "split_count = split_df.count()\n",
        "joined_count = joined_df.count()\n",
        "\n",
        "# Create a DataFrame containing the invalid rows\n",
        "invalid_df = split_df.join(F.broadcast(joined_df), 'folder', 'leftanti')\n",
        "\n",
        "# Validate the count of the new DataFrame is as expected\n",
        "invalid_count = invalid_df.count()\n",
        "print(\" split_df:\\t%d\\n joined_df:\\t%d\\n invalid_df: \\t%d\" % (split_count, joined_count, invalid_count))\n",
        "\n",
        "# Determine the number of distinct folder rows removed\n",
        "invalid_folder_count = invalid_df.select('folder').distinct().count()\n",
        "print(\"%d distinct invalid folders found\" % invalid_folder_count)"
      ],
      "metadata": {
        "id": "ZJaquv4f0anw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing\n",
        "\n",
        "schemas can be used for importing data, but they can also be used to simplify accessing information within pre-parsed data. If you're wondering why we didn't just define a full schema for the import, the Spark CSV parser is not capable of using complex schema types using lists."
      ],
      "metadata": {
        "id": "B0hMjL8W2H4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the dog details and show 10 untruncated rows\n",
        "print(joined_df.select('dog_list').show(10, truncate=False))\n",
        "\n",
        "# Define a schema type for the details in the dog list\n",
        "DogType = StructType([\n",
        "\tStructField(\"breed\", StringType(), False),\n",
        "    StructField(\"start_x\", IntegerType(), False),\n",
        "    StructField('start_y', IntegerType(), False),\n",
        "    StructField('end_x', IntegerType(), False),\n",
        "    StructField('end_y', IntegerType(), False)\n",
        "])"
      ],
      "metadata": {
        "id": "x_p1D2Zc2JBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis"
      ],
      "metadata": {
        "id": "rFBbV6Qg7z3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to return the number and type of dogs as a tuple\n",
        "def dogParse(doglist):\n",
        "  dogs = []\n",
        "  for dog in doglist:\n",
        "    (breed, start_x, start_y, end_x, end_y) = dog.split(',')\n",
        "    dogs.append((breed, int(start_x), int(start_y), int(end_x), int(end_y)))\n",
        "  return dogs\n",
        "\n",
        "# Create a UDF\n",
        "udfDogParse = F.udf(dogParse, ArrayType(DogType))\n",
        "\n",
        "# Use the UDF to list of dogs and drop the old column\n",
        "joined_df = joined_df.withColumn('dogs', udfDogParse('dog_list')).drop('dog_list')\n",
        "\n",
        "# Show the number of dogs in the first 10 rows\n",
        "joined_df.select(F.size('dogs')).show(10)"
      ],
      "metadata": {
        "id": "vDOlnN_P70PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(joined_df.schema.json())\n",
        "joined_df.printSchema()\n",
        "\n",
        "# Define a UDF to determine the number of pixels per image\n",
        "def dogPixelCount(doglist):\n",
        "  totalpixels = 0\n",
        "  for dog in doglist:\n",
        "    totalpixels += (dog[3] - dog[1]) * (dog[4] - dog[2])\n",
        "  return totalpixels\n",
        "\n",
        "# Define a UDF for the pixel count\n",
        "udfDogPixelCount = F.udf(dogPixelCount, IntegerType())\n",
        "joined_df = joined_df.withColumn('dog_pixels',udfDogPixelCount('dogs'))\n",
        "\n",
        "# Create a column representing the percentage of pixels\n",
        "joined_df = joined_df.withColumn('dog_percent', ('dog_pixels' / (joined_df.width*joined_df.height)) * 100)\n",
        "\n",
        "# Show the first 10 annotations with more than 60% dog\n",
        "joined_df.where('dog_percent > 60').show(10)"
      ],
      "metadata": {
        "id": "KTj10evv_shz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H5e8snL0BEL8"
      }
    }
  ]
}